{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "numeros = sklearn.datasets.load_digits()\n",
    "imagenes = numeros['images']  # Hay 1797 digitos representados en imagenes 8x8\n",
    "n_imagenes = len(imagenes)\n",
    "X = imagenes.reshape((n_imagenes, -1)) # para volver a tener los datos como imagen basta hacer data.reshape((n_imagenes, 8, 8))\n",
    "#X=imagenes\n",
    "Y = numeros['target']\n",
    "print(np.shape(X), np.shape(Y))\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = (scaler.transform(X_test))\n",
    "#conversion to pytorch tensor and adding a 1 dimension with unsqueeze\n",
    "X_train_t = torch.from_numpy(X_train.reshape((len(X_train), 8, 8))).unsqueeze(1)\n",
    "X_test_t = torch.from_numpy(X_test.reshape((len(X_test), 8, 8))).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8])\n",
      "torch.Size([898, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_t[0]))\n",
    "print(X_train_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(898, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f32b3254cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "plt.imshow(X_train.reshape((898, 8, 8))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 15, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(15, 1, kernel_size=(4, 4), stride=(1, 1), padding=(5, 5))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(1, 15, kernel_size=(4, 4), stride=(1, 1), padding=(5, 5))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): ConvTranspose2d(15, 1, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define el autoencoder\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        #input1, out:15\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 15, kernel_size=8, stride=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            # out:15, out7\n",
    "            torch.nn.Conv2d(15,1,kernel_size=4, stride=1,padding=5),\n",
    "            torch.nn.ReLU(True))\n",
    "        self.decoder = torch.nn.Sequential(             \n",
    "            torch.nn.ConvTranspose2d(1,15,kernel_size=4,padding=5),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(15,1,kernel_size=8, stride=1),\n",
    "            torch.nn.ReLU(True))\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "auto = Autoencoder()\n",
    "print(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializa modelo, loss y optimizador\n",
    "num_epochs = 100\n",
    "model = Autoencoder()\n",
    "distance = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1E-3)\n",
    "loss_values = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.9384\n",
      "epoch [2/100], loss:0.9375\n",
      "epoch [3/100], loss:0.9372\n",
      "epoch [4/100], loss:0.9371\n",
      "epoch [5/100], loss:0.9366\n",
      "epoch [6/100], loss:0.9359\n",
      "epoch [7/100], loss:0.9347\n",
      "epoch [8/100], loss:0.9328\n",
      "epoch [9/100], loss:0.9297\n",
      "epoch [10/100], loss:0.9248\n",
      "epoch [11/100], loss:0.9180\n",
      "epoch [12/100], loss:0.9094\n",
      "epoch [13/100], loss:0.9001\n",
      "epoch [14/100], loss:0.8921\n",
      "epoch [15/100], loss:0.8839\n",
      "epoch [16/100], loss:0.8740\n",
      "epoch [17/100], loss:0.8645\n",
      "epoch [18/100], loss:0.8547\n",
      "epoch [19/100], loss:0.8458\n",
      "epoch [20/100], loss:0.8389\n",
      "epoch [21/100], loss:0.8309\n",
      "epoch [22/100], loss:0.8224\n",
      "epoch [23/100], loss:0.8152\n",
      "epoch [24/100], loss:0.8081\n",
      "epoch [25/100], loss:0.8004\n",
      "epoch [26/100], loss:0.7923\n",
      "epoch [27/100], loss:0.7835\n",
      "epoch [28/100], loss:0.7742\n",
      "epoch [29/100], loss:0.7647\n",
      "epoch [30/100], loss:0.7558\n",
      "epoch [31/100], loss:0.7473\n",
      "epoch [32/100], loss:0.7387\n",
      "epoch [33/100], loss:0.7303\n",
      "epoch [34/100], loss:0.7227\n",
      "epoch [35/100], loss:0.7170\n",
      "epoch [36/100], loss:0.7124\n",
      "epoch [37/100], loss:0.7070\n",
      "epoch [38/100], loss:0.7017\n",
      "epoch [39/100], loss:0.6967\n",
      "epoch [40/100], loss:0.6913\n",
      "epoch [41/100], loss:0.6856\n",
      "epoch [42/100], loss:0.6798\n",
      "epoch [43/100], loss:0.6739\n",
      "epoch [44/100], loss:0.6690\n",
      "epoch [45/100], loss:0.6644\n",
      "epoch [46/100], loss:0.6594\n",
      "epoch [47/100], loss:0.6547\n",
      "epoch [48/100], loss:0.6501\n",
      "epoch [49/100], loss:0.6454\n",
      "epoch [50/100], loss:0.6410\n",
      "epoch [51/100], loss:0.6367\n",
      "epoch [52/100], loss:0.6327\n",
      "epoch [53/100], loss:0.6285\n",
      "epoch [54/100], loss:0.6248\n",
      "epoch [55/100], loss:0.6214\n",
      "epoch [56/100], loss:0.6179\n",
      "epoch [57/100], loss:0.6140\n",
      "epoch [58/100], loss:0.6103\n",
      "epoch [59/100], loss:0.6068\n",
      "epoch [60/100], loss:0.6047\n",
      "epoch [61/100], loss:0.6030\n",
      "epoch [62/100], loss:0.5964\n",
      "epoch [63/100], loss:0.5928\n",
      "epoch [64/100], loss:0.5907\n",
      "epoch [65/100], loss:0.5856\n",
      "epoch [66/100], loss:0.5827\n",
      "epoch [67/100], loss:0.5796\n",
      "epoch [68/100], loss:0.5760\n",
      "epoch [69/100], loss:0.5741\n",
      "epoch [70/100], loss:0.5709\n",
      "epoch [71/100], loss:0.5688\n",
      "epoch [72/100], loss:0.5672\n",
      "epoch [73/100], loss:0.5643\n",
      "epoch [74/100], loss:0.5629\n",
      "epoch [75/100], loss:0.5609\n",
      "epoch [76/100], loss:0.5588\n",
      "epoch [77/100], loss:0.5576\n",
      "epoch [78/100], loss:0.5555\n",
      "epoch [79/100], loss:0.5542\n",
      "epoch [80/100], loss:0.5532\n",
      "epoch [81/100], loss:0.5517\n",
      "epoch [82/100], loss:0.5503\n",
      "epoch [83/100], loss:0.5490\n",
      "epoch [84/100], loss:0.5475\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    output = model(X_train_t.float())\n",
    "    loss = distance(output, X_train_t.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_values[epoch] = loss.item()\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(1,num_epochs+1),loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform = model(X_train_t.float())\n",
    "latent_space = model.encoder(X_train_t.float())\n",
    "print(x_transform.size())\n",
    "print(latent_space.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra los resultados de las cuatro capas de convolucion\n",
    "plt.figure(figsize=(14,14))\n",
    "offset = 16\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+1) #imagenes originales\n",
    "    plt.imshow(X_train_t[i+offset][0].detach().numpy())\n",
    "    plt.title(Y_train[i+offset])\n",
    "    \n",
    "    j=0 # las imagenes reconstruidas por el autoencoder\n",
    "    plt.subplot(5,5,(i+1)+5*(j+1))\n",
    "    plt.imshow(x_transform[i+offset][0].detach().numpy())\n",
    "    \n",
    "    j=1 # una de las capas de la representacion latente\n",
    "    plt.subplot(5,5,(i+1)+5*(j+1))\n",
    "    plt.imshow(latent_space[i+offset][0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
